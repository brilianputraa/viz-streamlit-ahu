# ahu_query_lib: Complete Gap Analysis & Performance Bottlenecks

**Date:** 2026-01-07
**Project:** viz-streamlit-ahu
**Backend Library:** ahu_query_lib
**Analysis Purpose:** Identify ALL gaps preventing seamless integration with Streamlit viz app

---

## Table of Contents

1. [Critical Missing Features (P0)](#critical-missing-features-p0)
2. [Performance Bottlenecks](#performance-bottlenecks)
3. [API Design Issues](#api-design-issues)
4. [Data Format Mismatches](#data-format-mismatches)
5. [Recommended Implementation Roadmap](#recommended-implementation-roadmap)

---

## Critical Missing Features (P0)

### 1. Empty `daily_energy_summary` Table

**Current State:**
```python
# ahu_query_lib queries this table:
SELECT ... FROM ahu_data.daily_energy_summary
```

**Problem:**
- Table exists but has **0 rows**
- All data (815,140 rows) is in `ahu_data.energy_readings` (6-minute granularity)
- Library returns empty DataFrame for all queries

**Impact:** ‚ùå **CRITICAL** - No data can be retrieved through library

**Fix Required:**
```python
# Option A: Populate daily_energy_summary (one-time ETL)
# Option B: Change library to query energy_readings with GROUP BY
# Option C: Create materialized view (recommended for performance)
```

---

### 2. Column Name Mismatches

**Current Library Code:**
```python
# Library queries columns that don't exist:
"AVG(er.outdoor_temperature) as avg_outdoor_temp"
```

**Database Schema:**
```sql
-- Actual columns in energy_readings:
outdoor_temperature numeric,  -- NOT avg_outdoor_temp
outdoor_humidity numeric       -- NOT avg_outdoor_humidity
```

**Impact:** ‚ùå **CRITICAL** - All queries fail with `column does not exist`

**Required Fix:**
```python
# Change all library queries:
"AVG(er.outdoor_temperature) as outdoor_temperature"
"AVG(er.outdoor_humidity) as outdoor_humidity"

# OR create database view:
CREATE VIEW ahu_data.vw_energy_readings AS
SELECT ...,
       AVG(outdoor_temperature) as outdoor_temperature,
       AVG(outdoor_humidity) as outdoor_humidity
FROM ahu_data.energy_readings
```

---

### 3. No Indexes on `energy_readings`

**Current State:**
```sql
-- No indexes on timestamp or ahu_id columns
-- Every query does FULL TABLE SCAN on 815,140 rows
```

**Impact:** ‚ùå **CRITICAL** - Queries take 5+ seconds, database crashes under load

**Required Indexes:**
```sql
-- Essential indexes for performance:
CREATE INDEX CONCURRENTLY idx_energy_timestamp
    ON ahu_data.energy_readings(timestamp);

CREATE INDEX CONCURRENTLY idx_energy_ahu_time
    ON ahu_data.energy_readings(ahu_id, timestamp);

-- Covering index for daily queries (includes aggregated columns):
CREATE INDEX CONCURRENTLY idx_energy_daily_covering
    ON ahu_data.energy_readings(date_trunc('day', timestamp), ahu_id)
    INCLUDE (ccv_cold_water_kwh, hcv_steam_kwh, sf_electricity_kwh);

-- Partial index for recent data (commonly queried):
CREATE INDEX CONCURRENTLY idx_energy_recent
    ON ahu_data.energy_readings(timestamp DESC, ahu_id)
    WHERE timestamp >= CURRENT_DATE - INTERVAL '2 years';
```

**Performance Impact:**
- Before: 5000+ ms (full table scan)
- After: < 100 ms (index scan)

---

### 4. Missing Granular Cost Breakdown

**What Viz Program Needs:**
```python
# Korean column names used throughout app:
kWh_CCV, kWh_PC_CCV, kWh_HCV, kWh_DH_HCV
ÎπÑÏö©(Ïõê)_CCV, ÎπÑÏö©(Ïõê)_PC_CCV, ÎπÑÏö©(Ïõê)_HCV, ÎπÑÏö©(Ïõê)_DH_HCV
Ï†ÑÎ†•_ÎπÑÏö©(Ïõê), ÎÉâÏàò_ÎπÑÏö©(Ïõê), Ïä§ÌåÄ_ÎπÑÏö©(Ïõê), Ï¥ùÌï©_ÎπÑÏö©(Ïõê)
```

**What Library Returns:**
```python
# Library only returns summary columns:
cold_water_energy_kwh        # Should be split into CCV + PC_CCV
steam_energy_kwh             # Should be split into HCV + DH_HCV
electricity_energy_kwh       # Should be split into SF + other
chilled_water_cost_krw       # Should be split into CCV + PC_CCV
steam_cost_krw               # Should be split into HCV + DH_HCV
electricity_cost_krw         # Should be split into SF + other
```

**Impact:** ‚ùå **CRITICAL** - Requires 150+ lines of transformation code in adapter

**Required Enhancement:**
```python
# Add parameter to library functions:
def fetch_energy_consumption_cost(
    ahu_id: str,
    start_period: str,
    end_period: str,
    include_granular: bool = True,  # NEW PARAMETER
    include_korean_columns: bool = True  # NEW PARAMETER
) -> pd.DataFrame:
    """
    Returns DataFrame with columns:
    - If include_granular=False: summary columns (current behavior)
    - If include_granular=True:
        kWh_CCV, kWh_PC_CCV, kWh_HCV, kWh_DH_HCV
        kWh_AC_CCV, kWh_AC_HCV  # After-coil values
        ÎπÑÏö©(Ïõê)_CCV, ÎπÑÏö©(Ïõê)_PC_CCV, ÎπÑÏö©(Ïõê)_HCV, ÎπÑÏö©(Ïõê)_DH_HCV
        Ï†ÑÎ†•_ÎπÑÏö©(Ïõê), ÎÉâÏàò_ÎπÑÏö©(Ïõê), Ïä§ÌåÄ_ÎπÑÏö©(Ïõê), Ï¥ùÌï©_ÎπÑÏö©(Ïõê)
    """
```

---

### 5. Timezone Handling Inconsistency

**Current Behavior:**
```python
# Database returns timezone-aware timestamps:
2021-06-01 00:00:00+09:00

# But viz code expects timezone-naive:
2021-06-01 00:00:00
```

**Impact:** ‚ùå **CRITICAL** - `TypeError: Invalid comparison between dtype=datetime64[ns, UTC+09:00] and Timestamp`

**Required Fix:**
```python
# Add timezone handling parameter:
def fetch_energy_consumption_cost(
    ...,
    timezone_as: Literal["aware", "naive", "utc"] = "naive"
) -> pd.DataFrame:
    """
    timezone_as options:
    - "aware": Return timezone-aware timestamps (database default)
    - "naive": Return timezone-naive timestamps (for pandas compatibility)
    - "utc": Convert to UTC before returning
    """
    if timezone_as == "naive":
        df['period'] = df['period'].dt.tz_localize(None)
    return df
```

---

### 6. No Batch Query Support

**Current Limitation:**
```python
# Library only supports single AHU queries:
df = aql.fetch_energy_consumption_cost(ahu_id="AHU01", ...)
# To get all 45 AHUs, must loop 45 times!
```

**Impact:** ‚ö†Ô∏è **HIGH** - 45 separate queries = terrible performance

**Required Enhancement:**
```python
# Add batch query function:
def fetch_batch_energy_consumption(
    start_period: str,
    end_period: str,
    ahu_list: List[str] = None,  # None = all AHUs
    data_period: str = "daily",
    include_granular: bool = True
) -> pd.DataFrame:
    """
    Fetch energy data for multiple AHUs in a SINGLE query.

    Uses SQL: WHERE ahu_id IN (%s, %s, %s, ...) instead of looping

    Performance:
    - Current (looping): 45 queries √ó 100ms = 4.5 seconds
    - With batch: 1 query √ó 100ms = 0.1 seconds
    """
    query = """
        SELECT date_trunc('day', timestamp) as period, ahu_id,
               SUM(ccv_cold_water_kwh) as ccv_cold_water_kwh,
               ...
        FROM ahu_data.energy_readings
        WHERE timestamp >= %s AND timestamp < %s
          AND ahu_id IN %(ahu_list)s
        GROUP BY date_trunc('day', timestamp), ahu_id
    """
```

---

### 7. Missing Outdoor Air Data Function

**What Viz Program Needs:**
```python
# Two DataFrames:
df_oa_all   # High-resolution (6-minute) outdoor data
df_oa_daily # Daily averaged outdoor data
```

**Current Limitation:**
```python
# Library has fetch_sensor_data() but:
# 1. Returns ALL parameters mixed together
# 2. No dedicated outdoor air function
# 3. Requires manual filtering and aggregation
```

**Impact:** ‚ö†Ô∏è **HIGH** - Custom SQL needed in adapter

**Required Function:**
```python
def fetch_outdoor_air_data(
    start_date: str,
    end_date: str,
    aggregate: Literal["raw", "hourly", "daily"] = "raw",
    ahu_id: str = "AHU01"  # Use AHU01's outdoor sensor (same for all)
) -> pd.DataFrame:
    """
    Returns outdoor temperature and humidity data.

    Parameters:
    - aggregate: "raw" (6-min), "hourly", "daily"
    - Returns DataFrame with columns:
        [datetime, Ïô∏Í∏∞Ïò®ÎèÑ, Ïô∏Í∏∞ÏäµÎèÑ]  # Korean column names!
    """
```

---

### 8. Detail Data Format Mismatch

**What Viz Program Expects:**
```python
# Long format for detail sensor data:
columns = ["datetime", "Í≥µÏ°∞Í∏∞", "Ìï≠Î™©Î™Ö", "Í∞í"]
# Example rows:
2021-06-01 00:06:00, AHU01, CCV, 45.2
2021-06-01 00:06:00, AHU01, HCV, 12.8
2021-06-01 00:06:00, AHU01, SFST, 1
```

**What Library Returns:**
```python
# Wide format:
columns = ["timestamp", "ahu_id", "CCV", "HCV", "SFST", "RAT", "RAH", ...]
# Requires melt() transformation
```

**Impact:** ‚ö†Ô∏è **HIGH** - 50+ lines of transformation code needed

**Required Function:**
```python
def fetch_sensor_detail_long_format(
    ahu_id: str,
    start_date: str,
    end_date: str,
    parameters: List[str] = None  # None = all available
) -> pd.DataFrame:
    """
    Returns sensor data in viz-compatible long format.

    Returns DataFrame with columns:
        [datetime, Í≥µÏ°∞Í∏∞, Ìï≠Î™©Î™Ö, Í∞í]

    Ìï≠Î™©Î™Ö mapping:
        CCV ‚Üí CCV, HCV ‚Üí HCV, SFST ‚Üí SFST, RAT ‚Üí RAT, RAH ‚Üí RAH
    """
```

---

## Performance Bottlenecks

### Bottleneck #1: Full Table Scan on Every Query ‚ö†Ô∏è

**Problem:**
```sql
-- Current query execution plan:
EXPLAIN ANALYZE
SELECT date_trunc('day', timestamp), ahu_id, SUM(...)
FROM ahu_data.energy_readings
WHERE timestamp >= '2021-06-01'
GROUP BY date_trunc('day', timestamp), ahu_id;

-- Result: Seq Scan on energy_readings (cost=0.00..50000.00)
--        Actual: 5200.123 ms
```

**Root Cause:**
- No index on `timestamp` column
- Database must scan all 815,140 rows to find matching rows

**Solution:**
```sql
CREATE INDEX CONCURRENTLY idx_energy_timestamp
ON ahu_data.energy_readings(timestamp);
```

**Expected Improvement:**
- Before: 5000+ ms
- After: < 50 ms (100x faster)

---

### Bottleneck #2: GROUP BY on Unindexed Expression ‚ö†Ô∏è

**Problem:**
```sql
-- GROUP BY on function call prevents index usage:
GROUP BY date_trunc('day', timestamp)  -- Can't use index efficiently
```

**Root Cause:**
- Expression `date_trunc('day', timestamp)` must be computed for every row
- Index on `timestamp` alone doesn't help with GROUP BY

**Solution:**
```sql
-- 1. Add functional index (PostgreSQL 12+):
CREATE INDEX CONCURRENTLY idx_energy_day_ahu
ON ahu_data.energy_readings((date_trunc('day', timestamp)), ahu_id);

-- 2. OR use generated column:
ALTER TABLE ahu_data.energy_readings
ADD COLUMN day_timestamp date
    GENERATED ALWAYS AS (date_trunc('day', timestamp)::date) STORED;

CREATE INDEX idx_energy_day_ahu_gen
ON ahu_data.energy_readings(day_timestamp, ahu_id);
```

**Expected Improvement:**
- Before: HashAggregate (expensive)
- After: GroupAggregate (uses pre-sorted index)

---

### Bottleneck #3: N+1 Query Problem (Loop Over AHUs) ‚ö†Ô∏è

**Problem:**
```python
# Current adapter code:
for ahu_id in ahu_list:  # 45 iterations!
    df = aql.fetch_energy_consumption_cost(ahu_id=ahu_id, ...)
    all_data.append(df)
```

**Root Cause:**
- Each iteration = separate database connection + query
- 45 AHUs = 45 separate queries

**Solution:**
```python
# Use batch query (single connection, single query):
df = aql.fetch_batch_energy_consumption(
    start_period=start_date,
    end_period=end_date,
    ahu_list=ahu_list  # All AHUs in one query
)
```

**Expected Improvement:**
- Before: 45 queries √ó 100ms = 4.5 seconds
- After: 1 query √ó 100ms = 0.1 seconds (45x faster)

---

### Bottleneck #4: Missing Connection Pooling ‚ö†Ô∏è

**Problem:**
```python
# Library creates new connection for each query:
conn = psycopg2.connect(...)
# Query...
conn.close()
```

**Root Cause:**
- Connection establishment = expensive (~50ms overhead)
- No connection reuse

**Solution:**
```python
# Use connection pool:
from psycopg2 import pool

connection_pool = pool.SimpleConnectionPool(
    minconn=2,
    maxconn=10,
    host='localhost',
    port=5433,
    user='postgres',
    password='admin',
    database='ahu_monitoring'
)

def get_connection():
    return connection_pool.getconn()

def release_connection(conn):
    connection_pool.putconn(conn)
```

**Expected Improvement:**
- Before: 50ms connection overhead per query
- After: 0ms overhead (reuse existing connections)

---

### Bottleneck #5: Fetching All Columns When Only Few Needed ‚ö†Ô∏è

**Problem:**
```sql
-- Library queries all columns even if not needed:
SELECT * FROM ahu_data.energy_readings
-- Returns 30+ columns when only 5 are needed
```

**Root Cause:**
- SELECT * retrieves unnecessary data
- Increases memory usage and network transfer

**Solution:**
```sql
-- Explicit column selection:
SELECT
    date_trunc('day', timestamp) as period,
    ahu_id,
    ccv_cold_water_kwh,  -- Only needed columns
    hcv_steam_kwh,
    sf_electricity_kwh
FROM ahu_data.energy_readings
```

**Expected Improvement:**
- Before: Transfer 30 columns √ó 815k rows
- After: Transfer 5 columns √ó 815k rows (6x less data)

---

### Bottleneck #6: No Query Result Caching ‚ö†Ô∏è

**Problem:**
```python
# Same data fetched repeatedly:
# Query 1: Get data for 2021-06-01 to 2021-07-01
# Query 2: Get data for 2021-06-01 to 2021-07-01 (same!)
# Query 3: Get data for 2021-06-01 to 2021-07-01 (same!)
```

**Root Cause:**
- No caching mechanism
- Expensive queries repeated

**Solution:**
```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=128)
def fetch_energy_cached(query_hash: str, params: tuple):
    # Cache based on query hash
    return _execute_query(query_hash, params)

def fetch_energy_consumption_cost(
    ahu_id: str,
    start_period: str,
    end_period: str,
    use_cache: bool = True  # NEW PARAMETER
):
    cache_key = hashlib.md5(
        f"{ahu_id}_{start_period}_{end_period}".encode()
    ).hexdigest()

    if use_cache:
        return fetch_energy_cached(cache_key, (ahu_id, start_period, end_period))
    else:
        return _execute_query(...)
```

---

### Bottleneck #7: Large Result Sets Without Pagination ‚ö†Ô∏è

**Problem:**
```python
# Fetching all 815,140 rows at once:
df = aql.fetch_sensor_data(ahu_id="AHU01", aggregate="raw")
# Returns entire dataset into memory
```

**Root Cause:**
- No chunking/pagination
- High memory usage

**Solution:**
```python
def fetch_sensor_data_chunked(
    ahu_id: str,
    start_date: str,
    end_date: str,
    chunk_size: int = 10000,  # Rows per chunk
    aggregate: str = "raw"
) -> Iterator[pd.DataFrame]:
    """
    Yields DataFrames in chunks instead of loading all at once.

    Usage:
        for chunk in fetch_sensor_data_chunked(...):
            process_chunk(chunk)
    """
    offset = 0
    while True:
        query = """
            SELECT * FROM ahu_data.ahu_readings_staging
            WHERE timestamp >= %s AND timestamp < %s AND ahu_id = %s
            ORDER BY timestamp
            LIMIT %s OFFSET %s
        """
        chunk = pd.read_sql(query, conn, params=(start_date, end_date, ahu_id, chunk_size, offset))
        if chunk.empty:
            break
        yield chunk
        offset += chunk_size
```

---

### Bottleneck #8: Synchronous Blocking Queries ‚ö†Ô∏è

**Problem:**
```python
# Blocking call freezes UI:
df = aql.fetch_energy_consumption_cost(...)  # 5 seconds
# UI frozen during this time
```

**Root Cause:**
- Synchronous database calls
- No async support

**Solution:**
```python
import asyncio
import asyncpg

async def fetch_energy_consumption_cost_async(
    ahu_id: str,
    start_period: str,
    end_period: str
) -> pd.DataFrame:
    """
    Async version that doesn't block UI.
    """
    conn = await asyncpg.connect('postgresql://...')
    try:
        rows = await conn.fetch(query, ahu_id, start_period, end_period)
        return pd.DataFrame(rows)
    finally:
        await conn.close()

# Usage in async context:
df = await fetch_energy_consumption_cost_async(...)
```

---

## API Design Issues

### Issue #1: Inconsistent Parameter Naming

**Problem:**
```python
# Different functions use different names for same concept:
fetch_energy_consumption_cost(start_period=..., end_period=...)
fetch_sensor_data(start_date=..., end_date=...)
```

**Solution:**
```python
# Standardize all functions:
fetch_energy_consumption_cost(start_date=..., end_date=...)
fetch_sensor_data(start_date=..., end_date=...)
fetch_outdoor_air_data(start_date=..., end_date=...)
```

---

### Issue #2: No Type Hints

**Problem:**
```python
# No type information:
def fetch_energy_consumption_cost(ahu_id, start_period, end_period):
    # What types? What return type?
```

**Solution:**
```python
from typing import Literal, Optional
import pandas as pd

def fetch_energy_consumption_cost(
    ahu_id: str,
    start_period: str,  # YYYY-MM-DD format
    end_period: str,    # YYYY-MM-DD format
    data_period: Literal["daily", "hourly", "raw"] = "daily",
    include_granular: bool = False
) -> pd.DataFrame:
    """
    Fetch energy consumption and cost data for an AHU.

    Returns:
        DataFrame with columns:
        - period (datetime): Date/hour of the reading
        - ahu_id (str): AHU identifier
        - kWh_CCV (float): CCV cold water energy
        - ... (other columns based on include_granular)

    Raises:
        DatabaseConnectionError: If connection fails
        InvalidDateRangeError: If start_date > end_date
    """
```

---

### Issue #3: No Progress Callbacks for Long Queries

**Problem:**
```python
# Long query with no feedback:
df = aql.fetch_energy_consumption_cost(...)  # Takes 5 seconds
# User sees nothing, thinks app is frozen
```

**Solution:**
```python
from typing import Callable, Optional

def fetch_energy_consumption_cost(
    ...,
    progress_callback: Optional[Callable[[int, int], None]] = None
) -> pd.DataFrame:
    """
    progress_callback: Function called with (current, total) progress
        Example: lambda cur, total: st.progress(cur/total)
    """
    if progress_callback:
        progress_callback(0, 100)

    # Execute query in chunks
    for i in range(num_chunks):
        # ... fetch chunk ...
        if progress_callback:
            progress_callback((i+1) * 100 // num_chunks, 100)
```

---

### Issue #4: No Query Optimization Hints

**Problem:**
```python
# Library can't hint about query intent:
df = aql.fetch_energy_consumption_cost(...)
# Does user need all columns? All time range?
```

**Solution:**
```python
def fetch_energy_consumption_cost(
    ...,
    columns: Optional[List[str]] = None,  # Select specific columns
    optimize_for: Literal["speed", "memory"] = "speed"
) -> pd.DataFrame:
    """
    columns: If specified, only fetch these columns (faster)
    optimize_for:
        - "speed": Use more memory for faster queries
        - "memory": Use less memory, slower queries
    """
```

---

## Data Format Mismatches

### Mismatch #1: Date Format

**Library Returns:**
```python
period = "2021-06-01 00:00:00"  # datetime
```

**Viz Expects:**
```python
datetime = "2021-06-01"  # datetime
ÏãúÍ∞Ñ = "2021-06-01 00:00:00"  # datetime (separate column!)
Ïó∞ÎèÑ = 2021  # int (separate column!)
```

---

### Mismatch #2: AHU ID Format

**Library Returns:**
```python
ahu_id = "AHU1"  # No zero-padding
```

**Viz Expects:**
```python
Í≥µÏ°∞Í∏∞ = "AHU01"  # Zero-padded to 2 digits
```

---

### Mismatch #3: Column Language

**Library Returns:**
```python
# English column names:
["cold_water_energy_kwh", "steam_energy_kwh", ...]
```

**Viz Expects:**
```python
# Korean column names:
["kWh_CCV", "ÎπÑÏö©(Ïõê)_CCV", "Ï†ÑÎ†•_ÎπÑÏö©(Ïõê)", ...]
```

---

## Recommended Implementation Roadmap

### Phase 1: Critical Fixes (1-2 days) üî¥

**Priority: P0 - Must fix for basic functionality**

1. **Add indexes to `energy_readings`**
   ```sql
   CREATE INDEX CONCURRENTLY idx_energy_timestamp
       ON ahu_data.energy_readings(timestamp);
   CREATE INDEX CONCURRENTLY idx_energy_ahu_time
       ON ahu_data.energy_readings(ahu_id, timestamp);
   ```

2. **Fix column name mismatches**
   - Change all `avg_outdoor_temp` ‚Üí `outdoor_temperature`
   - Change all `avg_outdoor_humidity` ‚Üí `outdoor_humidity`

3. **Fix timezone handling**
   - Add `timezone_as` parameter to all functions
   - Default to `"naive"` for pandas compatibility

4. **Add batch query function**
   - `fetch_batch_energy_consumption()`
   - Single query for all AHUs

---

### Phase 2: Feature Parity (3-5 days) üü°

**Priority: P1 - Need for full compatibility**

1. **Add granular cost breakdown**
   - Add `include_granular=True` parameter
   - Return CCV/PC_CCV/HCV/DH_HCV separately
   - Add Korean column names option

2. **Add outdoor air function**
   - `fetch_outdoor_air_data()`
   - Support raw/hourly/daily aggregation
   - Return Korean column names

3. **Add detail data function**
   - `fetch_sensor_detail_long_format()`
   - Return long format for viz compatibility

---

### Phase 3: Performance Optimization (1 week) üü¢

**Priority: P2 - Nice to have**

1. **Add connection pooling**
   - Reuse connections instead of creating new ones

2. **Add query result caching**
   - LRU cache for repeated queries
   - Configurable TTL

3. **Add async support**
   - Non-blocking queries
   - Better UI responsiveness

4. **Add chunked fetching**
   - Iterator pattern for large datasets
   - Reduced memory usage

---

### Phase 4: API Improvements (1 week) üîµ

**Priority: P3 - Polish**

1. **Add type hints everywhere**
   - Full type coverage
   - Better IDE support

2. **Standardize parameter names**
   - Consistent naming across functions
   - Clear documentation

3. **Add progress callbacks**
   - Progress reporting for long queries
   - Cancel support

4. **Add query optimization hints**
   - Column selection
   - Speed/memory tradeoff

---

## Summary Statistics

| Category | Count | Status |
|----------|-------|--------|
| Critical Missing Features | 8 | ‚ùå Blocker |
| Performance Bottlenecks | 8 | ‚ö†Ô∏è Major |
| API Design Issues | 4 | ‚ö†Ô∏è Moderate |
| Data Format Mismatches | 3 | ‚ö†Ô∏è Moderate |
| **Total Issues** | **23** | |

---

## Quick Reference: Files to Modify

### In `ahu_query_lib`:
1. `/ahu-query-lib/ahu_query_lib/queries/energy_queries.py` - Add batch query, fix column names
2. `/ahu-query-lib/ahu_query_lib/db/connection.py` - Add connection pooling
3. `/ahu-query-lib/ahu_query_lib/api/energy.py` - Add timezone handling, granular options

### In Database:
1. Run index creation SQL (Phase 1, item 1)
2. Update column aliases if using views

### In `viz-streamlit-ahu`:
1. Keep `db_adapter.py` as temporary bridge
2. Migrate to fixed `ahu_query_lib` after Phase 1 complete

---

**Document Version:** 1.0
**Last Updated:** 2026-01-07
**Next Review:** After Phase 1 completion
