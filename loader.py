import os, glob, io, re, json
import pandas as pd
import chardet
from datetime import timedelta, datetime
import numpy as np
import streamlit as st
import hashlib 

# ===== ê²½ë¡œ =====
# Environment variable overrides (Windows paths as default for compatibility)
HISTORY_DIR = os.getenv("AHU_HISTORY_DIR", r"C:\Users\User\Desktop\history")
RESULT_BASE = os.getenv("AHU_RESULT_BASE", r"C:\Users\User\Desktop\ahu_app_results")

RAW_DIR   = os.path.join(RESULT_BASE, "raw_results")
FINAL_DIR = os.path.join(RESULT_BASE, "final_results")
OA_DIR    = os.path.join(RESULT_BASE, "oa_results")

for d in [RESULT_BASE, RAW_DIR, FINAL_DIR, OA_DIR]:
    os.makedirs(d, exist_ok=True)

META_FILE = os.path.join(FINAL_DIR, "processed_files.json")  # íŒŒì¼ëª…+mtime ë©”íƒ€

# ===== ë‹¨ê°€ (ì—°ë„ë³„) =====
ë‹¨ê°€_ë”•ì…”ë„ˆë¦¬ = {
    2022: {"ëƒ‰ìˆ˜ë‹¨ê°€": 295, "ì¦ê¸°ë‹¨ê°€": 52300, "ì „ê¸°ë‹¨ê°€": 119},
    2023: {"ëƒ‰ìˆ˜ë‹¨ê°€": 299, "ì¦ê¸°ë‹¨ê°€": 57500, "ì „ê¸°ë‹¨ê°€": 154},
    2024: {"ëƒ‰ìˆ˜ë‹¨ê°€": 304, "ì¦ê¸°ë‹¨ê°€": 61600, "ì „ê¸°ë‹¨ê°€": 168},
    2025: {"ëƒ‰ìˆ˜ë‹¨ê°€": 307, "ì¦ê¸°ë‹¨ê°€": 65000, "ì „ê¸°ë‹¨ê°€": 182}
}
def get_ë‹¨ê°€(year):
    return ë‹¨ê°€_ë”•ì…”ë„ˆë¦¬.get(year, {"ëƒ‰ìˆ˜ë‹¨ê°€": 300, "ì¦ê¸°ë‹¨ê°€": 60000, "ì „ê¸°ë‹¨ê°€": 150})

from common import (
    get_ë‹¨ê°€, í•­ëª©_ì—´ëŸ‰ë§µí•‘,
    ì„œí”Œë¼ì´íŒ¬ìš©ëŸ‰, í”„ë¡œì„¸ìŠ¤íŒ¬ìš©ëŸ‰, ë°°ê¸°íŒ¬ìš©ëŸ‰,
    ê¸°ì–´ëª¨í„°ìš©ëŸ‰, ë¡œí„°ëª¨í„°ìš©ëŸ‰,
    CDUìš©ëŸ‰, HEATERìš©ëŸ‰,
    ê±´ì‹ì œìŠµí˜•_ê³µì¡°ê¸°
)

def _resolve_device_and_power(ahu_base: str, tag: str):
    """
    tag ì˜ˆ: SFST, SFST1, SFST2, RFST1, EFST2, PC_SFST, COMPSS1, CDUSS, EHSS2 ...
    ë¦¬í„´: (ì¥ì¹˜ê·¸ë£¹, kW ìš©ëŸ‰)
    """
    t = str(tag).upper()

    # ì¥ì¹˜ ê·¸ë£¹
    if "SF" in t: group = "SF"
    elif "EF" in t: group = "EF"
    elif "RF" in t: group = "RF"
    elif "CDU" in t or "COMP" in t: group = "CDU/COMP"
    elif "EH" in t or "HT" in t: group = "EH"
    else: group = "ê¸°íƒ€"

    # ë²ˆí˜¸ í¬í•¨ëœ í‚¤ ì¶”ì¶œ (SFST2, RFST1, COMPSS1, CDUSS, EHSS2 ...)
    m = re.search(r'(SFST\d*|SFSS\d*|RFST\d*|RFSS\d*|EFST\d*|EFSS\d*|COMPSS\d*|COMP\d*|CDUSS|CDU|EHSS\d*|EH|HTSS|HT)', t)
    device_key = m.group(1) if m else t

    # ë²ˆí˜¸ í¬í•¨ í‚¤ ìš°ì„  ì¡°íšŒ â†’ ì—†ìœ¼ë©´ ê¸°ë³¸í‚¤ë¡œ fallback
    if group == "SF":
        kw = ì„œí”Œë¼ì´íŒ¬ìš©ëŸ‰.get((ahu_base, device_key))
        if kw is None: kw = ì„œí”Œë¼ì´íŒ¬ìš©ëŸ‰.get(ahu_base)
    elif group == "RF":
        kw = í”„ë¡œì„¸ìŠ¤íŒ¬ìš©ëŸ‰.get((ahu_base, device_key))
        if kw is None: kw = í”„ë¡œì„¸ìŠ¤íŒ¬ìš©ëŸ‰.get(ahu_base)
    elif group == "EF":
        kw = ë°°ê¸°íŒ¬ìš©ëŸ‰.get((ahu_base, device_key))
        if kw is None: kw = ë°°ê¸°íŒ¬ìš©ëŸ‰.get(ahu_base)
    elif group == "CDU/COMP":
        kw = CDUìš©ëŸ‰.get((ahu_base, device_key))
        if kw is None: kw = CDUìš©ëŸ‰.get((ahu_base, "COMP"))
    elif group == "EH":
        kw = HEATERìš©ëŸ‰.get((ahu_base, device_key))
        if kw is None: kw = HEATERìš©ëŸ‰.get((ahu_base, "HTSS"))
    else:
        kw = 0

    return group, float(kw or 0.0)


def ë³´ê°„_ì—´ëŸ‰ê³„ì‚°(df, í•­ëª©ëª…, ìµœëŒ€ì—´ëŸ‰, ì´ìƒê°’íƒì§€=True, midnight_only=True):
    df = df.sort_values("datetime").reset_index(drop=True)

    # 1. ì‹œê°„ ê°„ê²© ê³„ì‚°
    df["ì‹œê°„ê°„ê²©"] = df["datetime"].diff().dt.total_seconds() / 3600

    # 2. 00:00 ëˆ„ë½ ë³´ê°„ (ì˜µì…˜)
    if midnight_only:
        new_rows = []
        for day in pd.date_range(df["datetime"].min().normalize(),
                                 df["datetime"].max().normalize()):
            midnight = day
            if midnight not in df["datetime"].values:
                before = df[df["datetime"] < midnight].tail(1)
                after = df[df["datetime"] > midnight].head(1)
                if not before.empty and not after.empty:
                    val = before["ê°’"].iloc[0] + (after["ê°’"].iloc[0] - before["ê°’"].iloc[0]) * (
                        (midnight - before["datetime"].iloc[0]) /
                        (after["datetime"].iloc[0] - before["datetime"].iloc[0])
                    )
                    new_rows.append({"datetime": midnight, "ê³µì¡°ê¸°": df["ê³µì¡°ê¸°"].iloc[0],
                                     "í•­ëª©ëª…": í•­ëª©ëª…, "ê°’": val})
        if new_rows:
            df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True).sort_values("datetime")

    # 3. ìœ íš¨í•œ ì‹œê°„ ê°„ê²©ë§Œ ë‚¨ê¸°ê¸°
    df = df[(df["ì‹œê°„ê°„ê²©"] > 0) & (df["ì‹œê°„ê°„ê²©"] <= 12)].copy()

    # 4. ì—´ëŸ‰ ê³„ì‚° (trapezoid ë°©ì‹)
    v1, v2 = df["ê°’"].shift(1), df["ê°’"]
    df["ì—´ëŸ‰_kWh"] = ((v1 + v2) / 2) * ìµœëŒ€ì—´ëŸ‰ * df["ì‹œê°„ê°„ê²©"] / 100 / 860

    # 5. ì´ìƒê°’ íƒì§€
    if ì´ìƒê°’íƒì§€:
        df.loc[df["ì—´ëŸ‰_kWh"] > 300, "ì—´ëŸ‰_kWh"] = np.nan

    # 6. ë¹„ìš© ê³„ì‚° (ë‹¨ê°€ëŠ” ì—°ë„ë³„ë¡œ ë‚˜ì¤‘ì— ì ìš©, ì¼ë‹¨ placeholder)
    df["ë¹„ìš©(ì›)"] = np.nan

    return df


# ===== ê³µì¡°ê¸° íƒ€ì… =====
ê±´ì‹ì œìŠµí˜•_ê³µì¡°ê¸° = {"AHU03", "AHU07", "AHU09", "AHU11", "AHU14", "AHU021", "AHU023", "AHU025", "AHU026"}

# ===== CSV ë¡œë” =====
def _safe_read_bytes(path: str):
    try:
        with open(path, "rb") as f:
            return f.read()
    except Exception:
        return None

def _detect_encoding(b: bytes):
    det = chardet.detect(b)
    return det.get("encoding") or "utf-8"

def read_csv_fast(path: str) -> pd.DataFrame:
    raw = _safe_read_bytes(path)
    if not raw:
        return pd.DataFrame()
    enc = _detect_encoding(raw)
    try:
        iter_csv = pd.read_csv(
            io.BytesIO(raw),
            encoding=enc,
            chunksize=500_000,
            low_memory=False,
            dtype_backend="pyarrow",
        )
        return pd.concat(iter_csv, ignore_index=True)
    except Exception:
        return pd.DataFrame()

# ===== CSV íŒŒì„œ =====
def parse_ahu_csv(path: str) -> pd.DataFrame:
    df = read_csv_fast(path)
    if df.empty:
        return df

    # --- ì»¬ëŸ¼ëª… ì •ê·œí™” ---
    rename_map = {
        "Date": "datetime",
        "ë‚ ì§œ": "datetime",
        "date": "datetime",
        "Value": "ê°’",
        "value": "ê°’",
        "POINT": "point",
        "Point": "point",
    }
    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

    if "datetime" not in df.columns:
        raise ValueError(f"{path} íŒŒì¼ì— datetime ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ğŸš¨ ìƒë‹¨ ë¶ˆí•„ìš” í–‰ ì œê±°
    if "ê°’" in df.columns:
        mask_valid = (
            df["datetime"].notna() & df["datetime"].astype(str).str.strip().ne("") &
            df["ê°’"].notna() & df["ê°’"].astype(str).str.strip().ne("")
        )
        df = df[mask_valid].copy()

    if df.empty:
        return df

    # datetime ë¬¸ìì—´ ì²˜ë¦¬
    df["datetime"] = df["datetime"].astype(str).str.strip()
    try:
        df["datetime"] = pd.to_datetime(df["datetime"], format="%Y%m%d%H%M", errors="coerce")
    except Exception:
        df["datetime"] = pd.to_datetime(df["datetime"], errors="coerce")

    df = df.dropna(subset=["datetime", "ê°’"]).copy()

    # ê°’ ìˆ«ì ë³€í™˜
    df["ê°’"] = pd.to_numeric(df["ê°’"], errors="coerce")
    df = df.dropna(subset=["ê°’"])

    # --- ê³µì¡°ê¸°ëª… ì¶”ì¶œ ---
    fname = os.path.basename(path)
    fname_noext = os.path.splitext(fname)[0]
    ahu = re.sub(r"_\d+$", "", fname_noext.upper().split("-")[0])
    df["ê³µì¡°ê¸°"] = ahu

    # --- í•­ëª©ëª… ì¶”ì¶œ ---
    if "point" in df.columns:
        df["í•­ëª©ëª…"] = (
            df["point"].astype(str).str.upper()
            .str.replace(r"^AHU\d+_?", "", regex=True)
            .str.replace(r"\.PRESENTVALUE$", "", regex=True)
        )
    else:
        df["í•­ëª©ëª…"] = "UNKNOWN"

    # ğŸ”§ ì—¬ê¸°ë¶€í„° ì¶”ê°€ (ê³µë°±/ë³€í˜• ì •ë¦¬)
    df["í•­ëª©ëª…"] = df["í•­ëª©ëª…"].str.upper().str.strip()
    df["í•­ëª©ëª…"] = df["í•­ëª©ëª…"].str.replace(r"\s+(?=\d)", "", regex=True)   # 'SFST 1' -> 'SFST1'
    df["í•­ëª©ëª…"] = df["í•­ëª©ëª…"].str.replace(r"^AC_(CCV|HCV)$", r"\1", regex=True)  # 'AC_CCV'->'CCV'


    return df[["datetime", "ê³µì¡°ê¸°", "í•­ëª©ëª…", "ê°’"]]


def parse_oa_csv(path: str) -> pd.DataFrame:
    df = read_csv_fast(path)
    if df.empty or df.shape[1] < 3:
        return pd.DataFrame()
    df = df.iloc[:, :3]
    df.columns = ["label", "datetime", "value"]
    df["datetime"] = pd.to_datetime(df["datetime"].astype(str), errors="coerce")
    df = df.dropna(subset=["datetime"])
    df["value"] = pd.to_numeric(df["value"], errors="coerce")
    df_temp = df[df["label"].str.contains("OA_T", case=False, na=False)].rename(columns={"value": "ì™¸ê¸°ì˜¨ë„"})
    df_humi = df[df["label"].str.contains("OA_H", case=False, na=False)].rename(columns={"value": "ì™¸ê¸°ìŠµë„"})
    return pd.merge(
        df_temp[["datetime", "ì™¸ê¸°ì˜¨ë„"]],
        df_humi[["datetime", "ì™¸ê¸°ìŠµë„"]],
        on="datetime",
        how="outer",
    )

# ==== ì¥ì¹˜ ë¶„ë¥˜ ì„¸íŠ¸ & í—¬í¼ ====
# ì½”ì¼ ë³„ì¹­
COIL_ALIASES = {
    "CCV": {"CCV", "AC_CCV", "PC_CCV"},
    "HCV": {"HCV", "AC_HCV", "DH_HCV"},   # DH_HCVëŠ” ìŠ¤íŒ€ ì œìŠµ ì½”ì¼(ë³„ë„ ì§‘ê³„ ìœ ì§€)
}

# ì „ê¸°ë¶€í•˜ë§Œ ì—¬ê¸° í¬í•¨
MOTOR_SET = {
    "SFST","SFSS","EFST","EFSS","RFST",
    "AC_SFST","PC_SFST","OAU_SFST","AC_RFSS",
    "COMPST","CDU","CDUSST","COMP",
    "EHST","EHSS1","EHSS2","EHSS3",
    "DH_GMST",  # ì œìŠµ íœ  ê¸°ì–´ëª¨í„°
}

# ì„¼ì„œ/í‘œì‹œì¹˜(ë¹„ìš©Â·ìš´ì „ì‹œê°„ ìƒì„± ê¸ˆì§€)
SENSOR_SET = {
    "RAT","RAH","AC_RAT","AC_RAH","DH_DEH","DH_TEMP",
    "AC_SAT","PC_SAT","AC_SAH","PC_HCV"
}

def _token_from_col(c: str):
    """wide ì»¬ëŸ¼ì—ì„œ ì¥ì¹˜ í† í°ë§Œ ë½‘ëŠ” ìœ í‹¸"""
    if c.startswith("ë¹„ìš©(ì›)_"):      return c.split("_", 1)[1]
    if c.endswith("_ë¹„ìš©(ì›)"):        return c.rsplit("_", 1)[0]
    if c.startswith(("kWh_","kwh_")):  return c.split("_", 1)[1]
    if c.lower().endswith("_kwh"):     return c.rsplit("_", 1)[0]
    if c.startswith("ìš´ì „ì‹œê°„(h)_"):    return c.split("_", 1)[1]
    return None


# ===== ì—ë„ˆì§€/ë¹„ìš© ê³„ì‚° =====
@st.cache_data
def calculate_final_from_raw(raw_df: pd.DataFrame, min_dt=None, max_dt=None) -> pd.DataFrame:
    if raw_df.empty:
        return pd.DataFrame()

    # âœ… ì‹ ê·œ êµ¬ê°„ë§Œ ì˜ë¼ë‚´ê¸°
    if min_dt is not None and max_dt is not None:
        raw_df = raw_df[(raw_df["datetime"] >= min_dt) & (raw_df["datetime"] <= max_dt)].copy()
    if raw_df.empty:
        return pd.DataFrame()

    raw_df = raw_df.copy()
    raw_df["ì—°ë„"] = raw_df["datetime"].dt.year
    raw_df["ì‹œê°„"] = raw_df["datetime"].dt.floor("H")

    # ğŸ”§ ì¶”ê°€: í•­ëª©ëª… ì •ê·œí™”(2ì°¨ ë°©ì–´)
    raw_df["í•­ëª©ëª…"] = raw_df["í•­ëª©ëª…"].astype(str).str.upper().str.strip()
    raw_df["í•­ëª©ëª…"] = raw_df["í•­ëª©ëª…"].str.replace(r"\s+(?=\d)", "", regex=True)
    raw_df["í•­ëª©ëª…"] = raw_df["í•­ëª©ëª…"].str.replace(r"^AC_(CCV|HCV)$", r"\1", regex=True)


    results = []

    # ===== ì„¸ë¶€í•­ëª©ë³„ ê³„ì‚° =====
    for (ahu, year, í•­ëª©), grp in raw_df.groupby(["ê³µì¡°ê¸°", "ì—°ë„", "í•­ëª©ëª…"]):
        grp = grp.sort_values("datetime")
        grp["dt_h"] = grp["datetime"].diff().dt.total_seconds() / 3600
        grp = grp[(grp["dt_h"] > 0) & (grp["dt_h"] <= 5)].copy()
        if grp.empty:
            continue

        hourly = grp.groupby("ì‹œê°„", as_index=False).agg({"dt_h": "sum", "ê°’": "mean"})
        hourly["ì—°ë„"] = hourly["ì‹œê°„"].dt.year
        ë‹¨ê°€ = get_ë‹¨ê°€(hourly["ì—°ë„"].iloc[0])
        ahu_base = ahu.split("_")[0]

        item = í•­ëª©
        # === kWh / ë¹„ìš© ê³„ì‚° ===
        if item in ("CCV", "PC_CCV"):
            ìµœëŒ€ì—´ëŸ‰ = í•­ëª©_ì—´ëŸ‰ë§µí•‘[í•­ëª©].get(ahu_base, 0)
            hourly["kWh"] = hourly["ê°’"] * ìµœëŒ€ì—´ëŸ‰ * 0.01 * hourly["dt_h"] / 860
            hourly["ë¹„ìš©(ì›)"] = hourly["kWh"] * ë‹¨ê°€["ëƒ‰ìˆ˜ë‹¨ê°€"] * 860 / (2.3 * 4.187 * 1000)

            hourly["ê³µì¡°ê¸°"] = ahu_base
            hourly["í•­ëª©ëª…"] = í•­ëª©
            results.append(hourly[["ê³µì¡°ê¸°","ì‹œê°„","ì—°ë„","í•­ëª©ëª…","kWh","ë¹„ìš©(ì›)"]])

        elif item == "DH_HCV" or item.startswith("DH_HCV"):
            ìµœëŒ€ì—´ëŸ‰ = í•­ëª©_ì—´ëŸ‰ë§µí•‘[í•­ëª©].get(ahu_base, 0)
            hourly["kWh"] = hourly["ê°’"] * ìµœëŒ€ì—´ëŸ‰ * 0.01 * hourly["dt_h"] / 860
            hourly["ë¹„ìš©(ì›)"] = hourly["kWh"] * ë‹¨ê°€["ì¦ê¸°ë‹¨ê°€"] * 860 / (495 * 0.4 * 1000)

            hourly["ê³µì¡°ê¸°"] = ahu_base
            hourly["í•­ëª©ëª…"] = í•­ëª©
            results.append(hourly[["ê³µì¡°ê¸°","ì‹œê°„","ì—°ë„","í•­ëª©ëª…","kWh","ë¹„ìš©(ì›)"]])

        elif item in ("HCV", "AC_HCV"):
            ìµœëŒ€ì—´ëŸ‰ = í•­ëª©_ì—´ëŸ‰ë§µí•‘[í•­ëª©].get(ahu_base, 0)
            hourly["kWh"] = hourly["ê°’"] * ìµœëŒ€ì—´ëŸ‰ * 0.01 * hourly["dt_h"] / 860
            hourly["ë¹„ìš©(ì›)"] = hourly["kWh"] * ë‹¨ê°€["ì¦ê¸°ë‹¨ê°€"] * 860 / (540 * 0.4 * 1000)

            hourly["ê³µì¡°ê¸°"] = ahu_base
            hourly["í•­ëª©ëª…"] = í•­ëª©
            results.append(hourly[["ê³µì¡°ê¸°","ì‹œê°„","ì—°ë„","í•­ëª©ëª…","kWh","ë¹„ìš©(ì›)"]])

        elif any(key in item for key in ["SFST","PC_SFST","RFST","EFST","COMP","CDU","EH","HT","DH_EFST","DH_GMST"]):
            # â¬‡ï¸ ì—¬ê¸¸ ì „ë¶€ ì•„ë˜ ì½”ë“œë¡œ êµì²´
            ahu_base = ahu.split("_")[0]

            # ì‹œê³„ì—´ ì ë¶„: state(0/1) Ã— kW Ã— dt_h  (ì‚¬ë‹¤ë¦¬ê¼´)
            grp = grp.sort_values("datetime").copy()
            grp["dt_h"] = grp["datetime"].diff().dt.total_seconds() / 3600
            grp = grp[(grp["dt_h"] > 0) & (grp["dt_h"] <= 12)]

            # íƒœê·¸ë³„(kW) í•´ì„
            _, kw_cap = _resolve_device_and_power(ahu_base, í•­ëª©)

            v1 = grp["ê°’"].shift(1).fillna(0)
            v2 = grp["ê°’"].fillna(0)
            state_avg = ((v1 + v2) / 2.0).fillna(0)

            grp["kWh_seg"] = state_avg * kw_cap * grp["dt_h"]
            grp["run_seg"] = state_avg * grp["dt_h"]


            hourly = grp.groupby("ì‹œê°„", as_index=False).agg(
                kWh=("kWh_seg","sum"),
                ìš´ì „ì‹œê°„_h=("run_seg","sum")  # â† dt_h ë§ê³  run_seg í•©ì‚°
            )
            hourly.rename(columns={"ìš´ì „ì‹œê°„_h": "ìš´ì „ì‹œê°„(h)"}, inplace=True)

            hourly["ì—°ë„"] = hourly["ì‹œê°„"].dt.year
            hourly["ë¹„ìš©(ì›)"] = hourly["kWh"] * ë‹¨ê°€["ì „ê¸°ë‹¨ê°€"]
            hourly["ê³µì¡°ê¸°"] = ahu_base
            hourly["í•­ëª©ëª…"] = í•­ëª©
            results.append(hourly)
        

    if not results:
        return pd.DataFrame()

    detail_df = pd.concat(results, ignore_index=True)

    # ===== 2. í° í•­ëª©ë³„ (ëƒ‰ìˆ˜/ìŠ¤íŒ€/ì „ë ¥) í•©ì‚° =====
    group_map = {
    "ëƒ‰ìˆ˜": ["CCV", "PC_CCV", "AC_CCV"],
    "ìŠ¤íŒ€": ["HCV", "DH_HCV", "AC_HCV"],
    "ì „ë ¥": [
        "SFST", "SFST1", "SFST2", 
        "SFSS", "SFSS1", "SFSS2", # <- ì´ í•­ëª©ë“¤ì´ ëˆ„ë½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        "RFST", "RFST1", "RFST2", "RFSS", "RFSS1", "RFSS2",
        "EFST", "EFST1", "EFST2", "EFSS", "EFSS1", "EFSS2",
        "PC_SFST", "AC_SFST", "AC_RFSS", "OAU_SFST",
        "CDU", "CDUSST", "COMP", "COMPST", "COMPSS1", "COMPSS2",
        "EHST", "EHSS1", "EHSS2", "EHSS3",
        "DH_EFST", "DH_GMST"
    ]
}

    big_rows = []
    for (ê³µì¡°ê¸°, ì‹œê°„), grp in detail_df.groupby(["ê³µì¡°ê¸°", "ì‹œê°„"]):
        ì—°ë„ = grp["ì—°ë„"].iloc[0]
        for big, ì„¸ë¶€ in group_map.items():
            sub = grp[grp["í•­ëª©ëª…"].isin(ì„¸ë¶€)]
            if sub.empty:
                continue
            kWh = sub["kWh"].sum()
            ë¹„ìš© = sub["ë¹„ìš©(ì›)"].sum()
            big_rows.append(
                {
                    "ê³µì¡°ê¸°": ê³µì¡°ê¸°,
                    "ì‹œê°„": ì‹œê°„,
                    "ì—°ë„": ì—°ë„,
                    "í•­ëª©ëª…": big,
                    "kWh": kWh,
                    "ë¹„ìš©(ì›)": ë¹„ìš©,
                }
            )

    big_df = pd.DataFrame(big_rows)

    # ===== 3. ì´í•© (ëƒ‰ìˆ˜+ìŠ¤íŒ€+ì „ë ¥) =====
    total_rows = []
    for (ê³µì¡°ê¸°, ì‹œê°„), grp in big_df.groupby(["ê³µì¡°ê¸°", "ì‹œê°„"]):
        ì—°ë„ = grp["ì—°ë„"].iloc[0]
        kWh = grp["kWh"].sum()
        ë¹„ìš© = grp["ë¹„ìš©(ì›)"].sum()
        total_rows.append(
            {
                "ê³µì¡°ê¸°": ê³µì¡°ê¸°,
                "ì‹œê°„": ì‹œê°„,
                "ì—°ë„": ì—°ë„,
                "ì´í•©_kWh": kWh,
                "ì´í•©_ë¹„ìš©": ë¹„ìš©,
            }
        )
    total_df = pd.DataFrame(total_rows)

    # ===== 4. pivotìœ¼ë¡œ ì—´ ë‹¨ìœ„ ì •ë¦¬ =====
    values_cols = [c for c in ["kWh", "ë¹„ìš©(ì›)", "í‰ê·  ê°œë„ìœ¨(%)", "ìš´ì „ì‹œê°„(h)"] if c in detail_df.columns]

    pivot_detail = detail_df.pivot_table(
        index=["ê³µì¡°ê¸°", "ì‹œê°„", "ì—°ë„"],
        columns="í•­ëª©ëª…",
        values=values_cols,
        aggfunc="sum",
    )
    pivot_detail.columns = [f"{c1}_{c2}" for c1, c2 in pivot_detail.columns]
    pivot_detail = pivot_detail.reset_index()


    pivot_big = big_df.pivot_table(
    index=["ê³µì¡°ê¸°", "ì‹œê°„", "ì—°ë„"],
    columns="í•­ëª©ëª…",
    values=["kWh", "ë¹„ìš©(ì›)"],
    aggfunc="sum",
    )
    pivot_big.columns = [f"{c1}_{c2}" for c1, c2 in pivot_big.columns]
    pivot_big = pivot_big.reset_index()


    final_df = pivot_detail.merge(pivot_big, on=["ê³µì¡°ê¸°", "ì‹œê°„", "ì—°ë„"], how="outer")
    if not total_df.empty:
        final_df = final_df.merge(total_df, on=["ê³µì¡°ê¸°", "ì‹œê°„", "ì—°ë„"], how="outer")

    final_df["datetime"] = final_df["ì‹œê°„"]
    final_df = final_df.drop(columns=[c for c in final_df.columns if re.match(r'^(CCV|HCV).*ìš´ì „ì‹œê°„', c)], errors='ignore')
    #  A) ì„¼ì„œ ìœ ë˜ì˜ ë¹„ìš©/ìš´ì „ì‹œê°„ ì»¬ëŸ¼ ì œê±° (í˜¹ì‹œ ìƒê²¼ë‹¤ë©´ ë°©ì§€ì°¨ í•œ ë²ˆ ë”)
    bad_cols = [
        c for c in final_df.columns
        if (c.startswith("ë¹„ìš©(ì›)_") or c.endswith("_ë¹„ìš©(ì›)") or c.startswith("ìš´ì „ì‹œê°„(h)_"))
        and (_token_from_col(c) in SENSOR_SET)
    ]
    final_df.drop(columns=bad_cols, inplace=True, errors="ignore")

    #  B) ì „ë ¥/ì½”ì¼ ë¹„ìš© í‘œì¤€ ì»¬ëŸ¼ ë§Œë“¤ê¸° (ë³´ê¸°ìš© ë ˆì´ë¸” í†µì¼)
    # ì „ë ¥ í•©ì‚° ì»¬ëŸ¼ í‘œì¤€í™”
    if "ì „ë ¥_ë¹„ìš©(ì›)" in final_df.columns:
        pass  # ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    elif "ë¹„ìš©(ì›)_ì „ë ¥" in final_df.columns:
        final_df["ì „ë ¥_ë¹„ìš©(ì›)"] = final_df["ë¹„ìš©(ì›)_ì „ë ¥"]

    if "ë¹„ìš©(ì›)_ëƒ‰ìˆ˜" in final_df.columns:
        final_df["ëƒ‰ìˆ˜_ë¹„ìš©(ì›)"] = final_df["ë¹„ìš©(ì›)_ëƒ‰ìˆ˜"]
    if "ë¹„ìš©(ì›)_ìŠ¤íŒ€" in final_df.columns:
        final_df["ìŠ¤íŒ€_ë¹„ìš©(ì›)"] = final_df["ë¹„ìš©(ì›)_ìŠ¤íŒ€"]
    if "ì´í•©_ë¹„ìš©" in final_df.columns and "ì´í•©_ë¹„ìš©(ì›)" not in final_df.columns:
        final_df["ì´í•©_ë¹„ìš©(ì›)"] = final_df["ì´í•©_ë¹„ìš©"]

    #  C) ì „ë ¥_ë¹„ìš©(ì›) ì—†ê±°ë‚˜ ì „ë¶€ NaNì´ë©´ â†’ ëª¨í„° ë¹„ìš© í•©/ë³´ì •
    need_power_fill = ("ì „ë ¥_ë¹„ìš©(ì›)" not in final_df.columns) or final_df["ì „ë ¥_ë¹„ìš©(ì›)"].isna().all()

    if need_power_fill:
        # ëª¨í„° ë¹„ìš© ì»¬ëŸ¼ í•©ì‚° ì‹œë„
        motor_cost_cols = [
            c for c in final_df.columns
            if c.startswith("ë¹„ìš©(ì›)_") and (_token_from_col(c) in MOTOR_SET)
        ]
        if motor_cost_cols:
            final_df[motor_cost_cols] = final_df[motor_cost_cols].apply(pd.to_numeric, errors="coerce")
            final_df["ì „ë ¥_ë¹„ìš©(ì›)"] = final_df[motor_cost_cols].sum(axis=1, min_count=1)

    # ë¹„ìš© í•©ì‚°ë„ ì—†ìœ¼ë©´ kWh Ã— ë‹¨ê°€ë¡œ ë³´ì •
    if ("ì „ë ¥_ë¹„ìš©(ì›)" not in final_df.columns) or final_df["ì „ë ¥_ë¹„ìš©(ì›)"].isna().all():
        motor_kwh_cols = [
            c for c in final_df.columns
            if (_token_from_col(c) in MOTOR_SET) and (c.startswith("kWh_") or c.endswith("_kWh"))
        ]
        if motor_kwh_cols:
            final_df[motor_kwh_cols] = final_df[motor_kwh_cols].apply(pd.to_numeric, errors="coerce")
            kwh_sum = final_df[motor_kwh_cols].sum(axis=1, min_count=1)
            # ì „ê¸°ë‹¨ê°€ëŠ” ì—°ë„ë³„ ì ìš©
            price = final_df["ì—°ë„"].map(lambda y: get_ë‹¨ê°€(int(y))["ì „ê¸°ë‹¨ê°€"])
            final_df["ì „ë ¥_ë¹„ìš©(ì›)"] = kwh_sum * price

    #  D) í˜¹ì‹œ ëª¨í˜¸í•œ ì»¬ëŸ¼ ì¤‘ë³µì´ ìƒê²¼ë‹¤ë©´ ì •ë¦¬(ì„ íƒ)
    final_df = final_df.loc[:, ~final_df.columns.duplicated()]
    # === (=ì—¬ê¸°ê¹Œì§€ ì¶”ê°€)= ==========================================
    # ë¶ˆí•„ìš”í•œ ê³„ì‚° ì»¬ëŸ¼ ì œê±°
    drop_cols = ["DH_DEH_kWh", "DH_HCV_ìš´ì „ì‹œê°„(h)", "PC_CCV_ìš´ì „ì‹œê°„(h)"]
    final_df.drop(columns=[c for c in drop_cols if c in final_df.columns], inplace=True, errors="ignore")

    return final_df

# ===== íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ (íŒŒì¼ëª… + mtime) =====
def _get_files_signature(folder: str) -> dict:
    sig = {}
    for path in glob.glob(os.path.join(folder, "*.csv")):
        try:
            fname = os.path.basename(path)
            sig[fname] = os.path.getmtime(path)  # float (mtime)
        except FileNotFoundError:
            continue
    return sig

def load_processed_files() -> dict:
    if os.path.exists(META_FILE):
        try:
            with open(META_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return {k: float(v) for k, v in data.items()}
        except Exception:
            return {}
    return {}

@st.cache_data
def load_oa_daily():
    """
    OA ì¼í‰ê·  parquet(oa_daily_*.parquet) ë¡œë“œ
    ë°˜í™˜: datetime(ìì •), ì™¸ê¸°ì˜¨ë„, ì™¸ê¸°ìŠµë„
    """
    files = glob.glob(os.path.join(OA_DIR, "oa_daily_*.parquet"))
    if not files:
        return pd.DataFrame()

    df = pd.concat([pd.read_parquet(f, engine="pyarrow") for f in files], ignore_index=True)

    # ê¸°ì¡´ daily íŒŒì¼ì€ 'date' ì»¬ëŸ¼ ê¸°ì¤€ì´ë¯€ë¡œ ìì • datetimeìœ¼ë¡œ ë³€í™˜
    # (standardize_oaê°€ datetimeì„ ê¸°ëŒ€í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ë§ì¶°ì¤Œ)
    if "date" in df.columns and "datetime" not in df.columns:
        df["datetime"] = pd.to_datetime(df["date"]).astype("datetime64[ns]")
        df = df.drop(columns=["date"])

    # ìˆ«ìí™” ë°©ì–´
    for c in ["ì™¸ê¸°ì˜¨ë„", "ì™¸ê¸°ìŠµë„"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # ì •ë ¬/ì¤‘ë³µ ì •ë¦¬
    df = df.dropna(subset=["datetime"]).sort_values("datetime").drop_duplicates(subset=["datetime"])
    return df[["datetime", "ì™¸ê¸°ì˜¨ë„", "ì™¸ê¸°ìŠµë„"]]


def save_processed_files(signatures: dict):
    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(signatures, f, ensure_ascii=False, indent=2)


# ===== ë©”ì¸ (ì¦ë¶„ ìŠ¤ìº” + ì €ì¥) =====
def scan_and_update(progress_callback=None):
    # í˜„ì¬ íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ (íŒŒì¼ëª… + mtime)
    current_sig = _get_files_signature(HISTORY_DIR)

    # ê¸°ì¡´ ì‹œê·¸ë‹ˆì²˜ ë¡œë“œ
    processed_sig = load_processed_files()

    # ë³€ê²½ëœ íŒŒì¼ë§Œ ì¶”ë¦¬ê¸°
    files_to_process = [
        os.path.join(HISTORY_DIR, fname)
        for fname, mtime in current_sig.items()
        if processed_sig.get(fname) != mtime
    ]

    if not files_to_process:
        print("âš ï¸ ìƒˆë¡œìš´/ë³€ê²½ëœ íŒŒì¼ ì—†ìŒ â†’ parquetë§Œ ë¡œë“œ", flush=True)
        # ë³€ê²½ëœ íŒŒì¼ì´ ì—†ì„ ë•Œ
        return load_final_results(), load_oa_daily(), load_oa_results()


    raw_results, oa_results = [], []
    for i, path in enumerate(files_to_process):
        if progress_callback:
            progress_callback(i + 1, len(files_to_process), os.path.basename(path))
        base = os.path.basename(path).upper()

        if base.startswith("AHU"):
            df = parse_ahu_csv(path)
            if not df.empty:
                processed_rows = []
                for í•­ëª©ëª…, grp in df.groupby("í•­ëª©ëª…"):
                    ahu_base = grp["ê³µì¡°ê¸°"].iloc[0]
                    ìµœëŒ€ì—´ëŸ‰ = í•­ëª©_ì—´ëŸ‰ë§µí•‘.get(í•­ëª©ëª…, {}).get(ahu_base, 0)

                    # RAWì—ì„œëŠ” ë³´ê°„ë§Œ (ì—´ëŸ‰/ë¹„ìš© X)
                    grp = ë³´ê°„_ì—´ëŸ‰ê³„ì‚°(
                        grp, í•­ëª©ëª…, ìµœëŒ€ì—´ëŸ‰,
                        ì´ìƒê°’íƒì§€=False,
                        midnight_only=True
                    )
                    grp = grp[["datetime", "ê³µì¡°ê¸°", "í•­ëª©ëª…", "ê°’"]]
                    processed_rows.append(grp)

                df = pd.concat(processed_rows, ignore_index=True)
                raw_results.append(df)

        elif base.startswith("OA"):
            df = parse_oa_csv(path)
            if not df.empty:
                oa_results.append(df)

    # ===== AHU parquet ì €ì¥ (ì¦ë¶„) =====
    if raw_results:
        ahu_raw_df = pd.concat(raw_results, ignore_index=True)

        for ahu, grp in ahu_raw_df.groupby("ê³µì¡°ê¸°"):
            out_path_raw   = os.path.join(RAW_DIR,   f"analysis_results_{ahu}.parquet")
            out_path_final = os.path.join(FINAL_DIR, f"final_analysis_{ahu}.parquet")

            # âœ… ì´ë²ˆ ë°°ì¹˜(ìƒˆë¡œ ë“¤ì–´ì˜¨ ë°ì´í„°)ì˜ ì‹œê°„ ë²”ìœ„ë§Œ ê¸°ë¡
            new_min = grp["datetime"].min()
            new_max = grp["datetime"].max()

            # ---------- RAW ë³‘í•© ----------
            if os.path.exists(out_path_raw):
                old_lt = pd.read_parquet(out_path_raw, engine="pyarrow",
                                        filters=[("datetime", "<", new_min)])
                old_gt = pd.read_parquet(out_path_raw, engine="pyarrow",
                                        filters=[("datetime", ">", new_max)])
                combined = pd.concat([old_lt, grp, old_gt], ignore_index=True)
            else:
                combined = grp

            combined = (combined
                        .drop_duplicates(subset=["datetime","ê³µì¡°ê¸°","í•­ëª©ëª…","ê°’"], keep="last")
                        .sort_values("datetime"))
            combined.to_parquet(out_path_raw, index=False, engine="pyarrow")

            # ---------- FINAL ê³„ì‚° ----------
            # â¬…ï¸ ì—¬ê¸°! ì „ì²´ê°€ ì•„ë‹ˆë¼ 'ìƒˆ êµ¬ê°„'ë§Œ ê³„ì‚°
            final_delta = calculate_final_from_raw(combined, min_dt=new_min, max_dt=new_max)

            if not final_delta.empty:
                if os.path.exists(out_path_final):
                    old_lt = pd.read_parquet(out_path_final, engine="pyarrow",
                                            filters=[("datetime", "<", new_min)])
                    old_gt = pd.read_parquet(out_path_final, engine="pyarrow",
                                            filters=[("datetime", ">", new_max)])
                    old_final = pd.concat([old_lt, old_gt], ignore_index=True)
                    final_df = pd.concat([old_final, final_delta], ignore_index=True)
                    final_df = (final_df
                                .drop_duplicates(subset=["ì‹œê°„","ê³µì¡°ê¸°"], keep="last")
                                .sort_values("datetime"))
                else:
                    final_df = final_delta

                final_df = final_df.replace({pd.NA: np.nan})
                final_df.to_parquet(out_path_final, index=False, engine="pyarrow")


    else:
        print("âš ï¸ RAW parquet ì €ì¥ ì•ˆë¨ (raw_results ë¹„ì–´ìˆìŒ)", flush=True)

    # ===== OA parquet ì €ì¥ =====
    if oa_results:
        oa_raw_df = pd.concat(oa_results, ignore_index=True).drop_duplicates(subset=["datetime"])

        for year, grp in oa_raw_df.groupby(oa_raw_df["datetime"].dt.year):
            out_path_raw = os.path.join(OA_DIR, f"oa_results_{year}.parquet")
            if os.path.exists(out_path_raw):
                old_outside = pd.read_parquet(
                    out_path_raw, engine="pyarrow",
                    filters=[("datetime", "<", grp["datetime"].min())]
                )
                old_outside2 = pd.read_parquet(
                    out_path_raw, engine="pyarrow",
                    filters=[("datetime", ">", grp["datetime"].max())]
                )
                old = pd.concat([old_outside, old_outside2], ignore_index=True)
                grp = pd.concat([old, grp], ignore_index=True).drop_duplicates(subset=["datetime"])

            grp.to_parquet(out_path_raw, index=False, engine="pyarrow")

            # DAILY ì €ì¥
            daily = grp.copy()
            daily["date"] = daily["datetime"].dt.date
            daily_avg = daily.groupby("date", as_index=False)[["ì™¸ê¸°ì˜¨ë„", "ì™¸ê¸°ìŠµë„"]].mean(numeric_only=True)
            for c in ["ì™¸ê¸°ì˜¨ë„", "ì™¸ê¸°ìŠµë„"]:
                daily_avg[c] = pd.to_numeric(daily_avg[c], errors="coerce").round().astype("Int64")

            out_path_daily = os.path.join(OA_DIR, f"oa_daily_{year}.parquet")
            if os.path.exists(out_path_daily):
                old_outside = pd.read_parquet(
                    out_path_daily, engine="pyarrow",
                    filters=[("date", "<", daily_avg["date"].min())]
                )
                old_outside2 = pd.read_parquet(
                    out_path_daily, engine="pyarrow",
                    filters=[("date", ">", daily_avg["date"].max())]
                )
                old_daily = pd.concat([old_outside, old_outside2], ignore_index=True)
                daily_avg = pd.concat([old_daily, daily_avg], ignore_index=True).drop_duplicates(subset=["date"], keep="last")

            daily_avg.to_parquet(out_path_daily, index=False, engine="pyarrow")

    else:
        print("âš ï¸ OA parquet ì €ì¥ ì•ˆë¨ (oa_results ë¹„ì–´ìˆìŒ)", flush=True)

    save_processed_files(current_sig)
    print("DEBUG: processed_files.json ê°±ì‹  ì™„ë£Œ", flush=True)

    final_files = glob.glob(os.path.join(FINAL_DIR, "final_analysis_AHU*.parquet"))
    final_df = pd.DataFrame()
    if final_files:
        final_df = pd.concat([pd.read_parquet(f) for f in final_files])
        final_df["datetime"] = pd.to_datetime(final_df["datetime"])

    oa_results_df = load_oa_results()   # ê³ í•´ìƒë„
    oa_daily_df   = load_oa_daily()     # ì¼í‰ê· 

    # â¬‡ï¸ ë‘ ë²ˆì§¸ ë¦¬í„´ê°’ì„ ì´ì œ dailyë¡œ!
    return final_df, oa_daily_df, oa_results_df


# ===== detail ë¡œë” =====
def load_ahu_detail(ahu_name: str) -> pd.DataFrame:
    """
    íŠ¹ì • ê³µì¡°ê¸°ì˜ detail parquet ì½ê¸°
    """
    exact = os.path.join(RAW_DIR, f"analysis_results_{ahu_name}.parquet")
    if not os.path.exists(exact):
        return pd.DataFrame()
    return pd.read_parquet(exact, engine="pyarrow")

# ===== ë¡œë” =====
@st.cache_data
def load_final_results():
    """
    ìµœì¢… ì§‘ê³„(FINAL) parquetì„ ëª¨ë‘ ì½ì–´ì™€ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹¨
    (ì „ì²´ ë°ì´í„° ë¡œë“œê°€ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©)
    """
    files = glob.glob(os.path.join(FINAL_DIR, "final_analysis_AHU*.parquet"))
    if not files:
        return pd.DataFrame()
    return pd.concat(
        [pd.read_parquet(f, engine="pyarrow") for f in files],
        ignore_index=True
    )

@st.cache_data
def load_final_by_ahu(ahu_name: str) -> pd.DataFrame:
    """
    íŠ¹ì • ê³µì¡°ê¸°(AHU)ì˜ ìµœì¢… ì§‘ê³„(FINAL) parquetë§Œ ì½ì–´ì˜´
    """
    path = os.path.join(FINAL_DIR, f"final_analysis_{ahu_name}.parquet")
    if not os.path.exists(path):
        return pd.DataFrame()
    return pd.read_parquet(path, engine="pyarrow")

def load_detail_results(force_recalc=False):
    """
    RAW detail parquet ì½ê¸° â†’ ê¸°ë³¸ì ìœ¼ë¡œëŠ” ì‚¬ìš© ì•ˆ í•˜ê³ 
    í•„ìš”í•  ë•Œë§Œ í˜¸ì¶œ
    """
    files = glob.glob(os.path.join(RAW_DIR, "analysis_results_AHU*.parquet"))
    if not files:
        return pd.DataFrame()
    return pd.concat(
        [pd.read_parquet(f, engine="pyarrow") for f in files],
        ignore_index=True
    )

@st.cache_data
def load_oa_results():
    """
    OA parquet â†’ í•­ìƒ ë¶ˆëŸ¬ì˜¤ê¸°
    """
    files = glob.glob(os.path.join(OA_DIR, "oa_results_*.parquet"))
    if not files:
        return pd.DataFrame()
    return pd.concat(
        [pd.read_parquet(f, engine="pyarrow") for f in files],
        ignore_index=True
    )

# ===== ìœ í‹¸ í•¨ìˆ˜ =====
def get_items_from_final(final_df: pd.DataFrame):
    """
    FINAL parquetì—ì„œ í•­ëª©ëª… ëª©ë¡ ì¶”ì¶œ
    (ì˜ˆ: kWh_CCV â†’ CCV, ë¹„ìš©(ì›)_HCV â†’ HCV)
    """
    if final_df.empty:
        return []

    items = []
    for col in final_df.columns:
        if "_" in col and not col.startswith("ì´í•©"):
            items.append(col.split("_")[-1])
    return sorted(set(items))


def update_history_results(progress_callback=None):
    # í˜¸í™˜ìš© â†’ ë‚´ë¶€ì ìœ¼ë¡œ scan_and_update í˜¸ì¶œ
    return scan_and_update(progress_callback)

# ===== í˜¸í™˜ìš© í•¨ìˆ˜ (app2.pyì™€ ì—°ê²°) =====
def load_or_calculate_results():
    return load_final_results()